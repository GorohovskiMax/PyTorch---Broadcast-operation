{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUrci-_ibe8I"
      },
      "source": [
        "Submitted by: **Maxim Gorohovski**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUqYaytEbrOY"
      },
      "source": [
        "# üìåImplementation of 'Broadcasting' Functionality in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM1R2hDScDiD"
      },
      "source": [
        "## üìñ **Background**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHjfvDTecF4j"
      },
      "source": [
        "Broadcasting is a mechanism that allows operations between tensors of different shapes. The mechanism works by automatically expanding one or both tensors to have compatible dimensions.  \n",
        "\n",
        "In pure mathematics, such as linear algebra, operations like **addition** or **multiplication** require that vectors or matrices will strict follow dimensional rules. But in the case of PyTorch, broadcasting is extremely useful for multiple reasons:\n",
        "\n",
        "- üöÄ **No need for 'for' loops**: Before broadcasting, we would've generally approached the element-wise operations by running loops. Since we are working with multiple multiplications and additions, we would have to repeat these operations many times, and that of course would mean that the program's execution would be pretty slow. Using broadcasting would mean that no loops are actually needed and therefore, our program will be a lot faster and a lot more efficient.\n",
        "\n",
        "- üöÄ **Saves Memory**: Broadcasting saves memory, because it avoids unnecessary memory duplication. Without broadcasting, we would've had to explictly expand and store vectors and matrices before applying any operations. But what broadcasting does is, it uses logical expansion to expand smaller tensors to fit the right dimensions, so essentially we don't allocate any extra memory - which is highly efficient!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7N87-_DPw9T"
      },
      "source": [
        "## üî® Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7UnrcM4IKr-"
      },
      "source": [
        "We will implement broadcasting using the following set of rules:\n",
        "\n",
        "- When comparing dimensions from **right** to **left**\n",
        "  - Dimensions must be either:\n",
        "    1. **equal**\n",
        "    2. One of the dimensions is 1\n",
        "\n",
        "- The resulting dimensions after broadcasting is the **maximum** of the two compared dimensions.\n",
        "\n",
        "- If these conditions fail, broadcasting will raise an error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1xgj11aWM0mf"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFD0EMlPQBFe"
      },
      "source": [
        "1Ô∏è‚É£ **expand_as** function (a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2R6RSyDbbdCs"
      },
      "outputs": [],
      "source": [
        "def expand_as(tensorA,tensorB):\n",
        "  # Creating a clone of tensorA to not modify the original tensor\n",
        "  tensorC = tensorA.clone()\n",
        "\n",
        "  # Smaller amount of dimensions in A, we must add singleton dimensions to the leftmost dimension of tensorC\n",
        "  while len(tensorC.shape) < len(tensorB.shape):\n",
        "      tensorC = tensorC.unsqueeze(0)\n",
        "\n",
        "  repeat_dims = [] # How many times each dimensions needs to be repeated\n",
        "  for dimA,dimB in zip(reversed(tensorC.shape),reversed(tensorB.shape)):\n",
        "    if dimA == dimB:\n",
        "      repeat_dims.insert(0,1)\n",
        "    elif dimA == 1: # We want to expand tensorC dimension to be the tensorB's dimension\n",
        "      repeat_dims.insert(0, dimB)\n",
        "    else:\n",
        "      raise ValueError(f\"dimension mismatch - tensorC.shape[{i}] = {tensorC.shape[i]} can't be expanded to tensorB.shape[{i}] = {tensorB.shape[i]}\")\n",
        "\n",
        "  for i,repeats in enumerate(repeat_dims): #for every index and number of repeats, we duplicate the tensor on the i-th dimension\n",
        "    if repeats != 1:\n",
        "      tensorC = torch.cat([tensorC] * repeats, dim=i)\n",
        "\n",
        "  return tensorC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO9eOQT_QP0f"
      },
      "source": [
        "2Ô∏è‚É£ **mutually_expandable** function (b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48JvktswxEBS"
      },
      "outputs": [],
      "source": [
        "def mutually_expandable(tensorA,tensorB):\n",
        "  # No need for clones, simply work with the shapes themselves\n",
        "  shapeA = list(tensorA.shape)\n",
        "  shapeB = list(tensorB.shape)\n",
        "  broadcast_shape = [] # Here we keep the final shape\n",
        "\n",
        "  # Making sure both tensors have the same number of dimensions\n",
        "  while len(shapeA) < len(shapeB):\n",
        "    shapeA.insert(0,1) # add singleton dimensions to the leftmost dimension of tensorA\n",
        "  while len(shapeB) < len(shapeA):\n",
        "    shapeB.insert(0,1)\n",
        "\n",
        "  for dimA, dimB in zip(reversed(shapeA),reversed(shapeB)):\n",
        "    if dimA == dimB:\n",
        "      broadcast_shape.append(dimA)\n",
        "\n",
        "    elif dimA == 1:\n",
        "      broadcast_shape.append(dimB) # We want to increase the dimension of tensorC\n",
        "\n",
        "    elif dimB == 1:\n",
        "      broadcast_shape.append(dimA) # We want to increase the dimension of tensorD\n",
        "\n",
        "    else:\n",
        "      return False, None\n",
        "\n",
        "  broadcast_shape.reverse() # Because we went from right to left in the 'for' loop, but the shape should be from left to right\n",
        "  return True, tuple(broadcast_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKAOVmGGQX3t"
      },
      "source": [
        "3Ô∏è‚É£ **mutually_broadcast** function (c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s5P4aAK5iVo"
      },
      "outputs": [],
      "source": [
        "def mutually_broadcast(tensorA,tensorB):\n",
        "  is_expandable, broadcast_shape = mutually_expandable(tensorA,tensorB)\n",
        "  if is_expandable:\n",
        "    dummy_tensor = torch.empty(broadcast_shape) # dummy-target tensor for A and B\n",
        "    tensorC = expand_as(tensorA,dummy_tensor)\n",
        "    tensorD = expand_as(tensorB,dummy_tensor)\n",
        "    return tensorC, tensorD\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"Can't mutually broadcast because {tensorA.shape} and {tensorB.shape} are not mutually-expandable\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdaGey-nLdqw"
      },
      "source": [
        "# üß™Tester"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsqq6Rz0KY7F"
      },
      "source": [
        "A tester generated by AI, it covers all possible test cases including failure cases, and tests all those cases on all of the functions we've implemented. We then compare the results with the original PyTorch functions to make sure they function the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dgWRwBIoLdKE"
      },
      "outputs": [],
      "source": [
        "def broadcast_test():\n",
        "    test_cases = [\n",
        "        (torch.tensor(5), torch.tensor([[1, 2], [3, 4]])),\n",
        "        (torch.tensor(1), torch.zeros(1)),\n",
        "        (torch.tensor(42), torch.zeros(3, 1, 5)),\n",
        "        (torch.tensor([1, 2, 3]), torch.zeros(3, 3)),\n",
        "        (torch.tensor([1]), torch.zeros(2, 2)),\n",
        "        (torch.ones(3, 4), torch.zeros(3, 4)),\n",
        "        (torch.ones(1, 4), torch.zeros(3, 4)),\n",
        "        (torch.ones(3, 1), torch.zeros(3, 5)),\n",
        "        (torch.ones(1, 3, 1, 5), torch.zeros(2, 3, 4, 5)),\n",
        "        (torch.ones(2, 1, 4), torch.zeros(2, 3, 4)),\n",
        "        (torch.ones(1, 1, 4, 1), torch.zeros(2, 3, 4, 5)),\n",
        "        # Broadcasting failure expected:\n",
        "        (torch.zeros(2, 3), torch.zeros(4, 2)),\n",
        "        (torch.zeros(2, 1, 3), torch.zeros(1, 3, 4)),\n",
        "    ]\n",
        "\n",
        "    print(\"Running full broadcast tests...\\n\")\n",
        "    for idx, (A, B) in enumerate(test_cases):\n",
        "        print(f\"--- Test Case #{idx + 1} ---\")\n",
        "        print(f\"A.shape: {A.shape}, B.shape: {B.shape}\")\n",
        "\n",
        "        # 1Ô∏è‚É£ Test mutually_expandable\n",
        "        try:\n",
        "            expected_shape = torch.broadcast_shapes(A.shape, B.shape)\n",
        "            expandable, shape = mutually_expandable(A, B)\n",
        "            assert expandable, \"mutually_expandable returned False unexpectedly\"\n",
        "            assert shape == expected_shape, f\"Shape mismatch: expected {expected_shape}, got {shape}\"\n",
        "            print(\"‚úÖ mutually_expandable: PASSED\")\n",
        "        except RuntimeError:\n",
        "            expandable, shape = mutually_expandable(A, B)\n",
        "            assert not expandable and shape is None, \"Expected failure, but got expandable\"\n",
        "            print(\"‚úÖ mutually_expandable: Correctly failed\")\n",
        "\n",
        "        # 2Ô∏è‚É£ Test expand_as (only if broadcasting possible)\n",
        "        try:\n",
        "            torch_result = A.expand_as(B)\n",
        "            custom_result = expand_as(A, B)\n",
        "            assert torch.equal(custom_result, torch_result), \"expand_as result mismatch\"\n",
        "            print(\"‚úÖ expand_as: PASSED\")\n",
        "        except RuntimeError:\n",
        "            try:\n",
        "                custom_result = expand_as(A, B)\n",
        "                assert False, \"Custom expand_as should have failed but didn't\"\n",
        "            except Exception:\n",
        "                print(\"‚úÖ expand_as: Correctly failed\")\n",
        "\n",
        "        # 3Ô∏è‚É£ Test mutually_broadcast\n",
        "        try:\n",
        "            my_C, my_D = mutually_broadcast(A, B)\n",
        "            torch_C, torch_D = torch.broadcast_tensors(A, B)\n",
        "            assert torch.equal(my_C, torch_C), \"broadcast_tensors result mismatch (C)\"\n",
        "            assert torch.equal(my_D, torch_D), \"broadcast_tensors result mismatch (D)\"\n",
        "            print(\"‚úÖ mutually_broadcast: PASSED\")\n",
        "        except ValueError:\n",
        "            try:\n",
        "                torch.broadcast_tensors(A, B)\n",
        "                assert False, \"Expected PyTorch broadcast_tensors to fail, but it didn't\"\n",
        "            except RuntimeError:\n",
        "                print(\"‚úÖ mutually_broadcast: Correctly failed\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(\"All tests passed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WboyhPaxMbwn",
        "outputId": "7fc71e53-c15a-4081-ac02-8bb8303727d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running full broadcast tests...\n",
            "\n",
            "--- Test Case #1 ---\n",
            "A.shape: torch.Size([]), B.shape: torch.Size([2, 2])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #2 ---\n",
            "A.shape: torch.Size([]), B.shape: torch.Size([1])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #3 ---\n",
            "A.shape: torch.Size([]), B.shape: torch.Size([3, 1, 5])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #4 ---\n",
            "A.shape: torch.Size([3]), B.shape: torch.Size([3, 3])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #5 ---\n",
            "A.shape: torch.Size([1]), B.shape: torch.Size([2, 2])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #6 ---\n",
            "A.shape: torch.Size([3, 4]), B.shape: torch.Size([3, 4])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #7 ---\n",
            "A.shape: torch.Size([1, 4]), B.shape: torch.Size([3, 4])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #8 ---\n",
            "A.shape: torch.Size([3, 1]), B.shape: torch.Size([3, 5])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #9 ---\n",
            "A.shape: torch.Size([1, 3, 1, 5]), B.shape: torch.Size([2, 3, 4, 5])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #10 ---\n",
            "A.shape: torch.Size([2, 1, 4]), B.shape: torch.Size([2, 3, 4])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #11 ---\n",
            "A.shape: torch.Size([1, 1, 4, 1]), B.shape: torch.Size([2, 3, 4, 5])\n",
            "‚úÖ mutually_expandable: PASSED\n",
            "‚úÖ expand_as: PASSED\n",
            "‚úÖ mutually_broadcast: PASSED\n",
            "\n",
            "--- Test Case #12 ---\n",
            "A.shape: torch.Size([2, 3]), B.shape: torch.Size([4, 2])\n",
            "‚úÖ mutually_expandable: Correctly failed\n",
            "‚úÖ expand_as: Correctly failed\n",
            "‚úÖ mutually_broadcast: Correctly failed\n",
            "\n",
            "--- Test Case #13 ---\n",
            "A.shape: torch.Size([2, 1, 3]), B.shape: torch.Size([1, 3, 4])\n",
            "‚úÖ mutually_expandable: Correctly failed\n",
            "‚úÖ expand_as: Correctly failed\n",
            "‚úÖ mutually_broadcast: Correctly failed\n",
            "\n",
            "All tests passed successfully!\n"
          ]
        }
      ],
      "source": [
        "broadcast_test()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
